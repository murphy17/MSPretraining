{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.baselines import CNNModel, CARPModel, MSModel, LinearModel\n",
    "from src.torch_helpers import NamedTensorDataset\n",
    "from src.datamodule import PeptideDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from src.torch_helpers import NoValProgressBar\n",
    "from src.constants import MSConstants\n",
    "C = MSConstants()\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.torch_helpers import start_tensorboard\n",
    "# start_tensorboard(login_node='login-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate negatives by producing tons of shuffled sequences\n",
    "# then cluster, and only take clusters containing sufficiently many positives\n",
    "\n",
    "# from src.cdhit import CDHIT\n",
    "\n",
    "# def generate_negatives(sequences, num_shuffles=10, min_frac=0.1, random_state=0):\n",
    "#     pos_seqs = list(sequences)\n",
    "#     neg_seqs = []\n",
    "#     rng = npr.RandomState(random_state)\n",
    "#     for n in range(num_shuffles):\n",
    "#         neg_seqs += [''.join(rng.permutation(list(s))) for s in pos_seqs]\n",
    "\n",
    "#     seqs = np.array(pos_seqs + neg_seqs)\n",
    "#     ids = np.array([1]*len(pos_seqs) + [0]*len(neg_seqs))\n",
    "#     clusters = np.array(CDHIT(threshold=0.5,word_length=3).fit_predict(seqs))\n",
    "#     pos_frac = pd.DataFrame([clusters,ids],index=['clusters','ids']).T.groupby('clusters').mean()['ids']\n",
    "#     pos_clusters = set(pos_frac[pos_frac>min_frac].index)\n",
    "    \n",
    "#     pos_seqs = set(pos_seqs)\n",
    "#     negatives = [s for i,s,c in zip(ids,seqs,clusters) if i==0 and c in pos_clusters and s not in pos_seqs]\n",
    "    \n",
    "#     rng.shuffle(negatives)\n",
    "#     negatives = negatives[:len(sequences)]\n",
    "    \n",
    "#     assert len(negatives) == len(sequences)\n",
    "    \n",
    "#     return negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitochondrial targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/mitochondria_targeting.csv')\n",
    "df = df[['Sequence','Mitochondrial Targeting Signal']].drop_duplicates(keep='first')\n",
    "df = df.loc[df['Sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['Sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df[['Mitochondrial Targeting Signal']].astype(np.int32).values\n",
    ")\n",
    "\n",
    "datasets['mito'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cdc28 binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cdc28_binding.csv')\n",
    "df = df[['Sequence','Cdc28 Binding']].drop_duplicates(keep='first')\n",
    "df = df.loc[df['Sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['Sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df[['Cdc28 Binding']].astype(np.int32).values\n",
    ")\n",
    "\n",
    "datasets['cdc28'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_set.fasta','r') as f:\n",
    "    fasta = [l.strip() for l in f]\n",
    "    df = pd.Series(fasta[::3]).str.extract(\n",
    "        '>(?P<uniprot>[^\\|]+)\\|(?P<kingdom>[^|]+)\\|(?P<type>[^|]+)\\|(?P<partition>[^|]+)'\n",
    "    )\n",
    "    df['sequence'] = fasta[1::3]\n",
    "    df['annotation'] = fasta[2::3]\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['type']!='NO_SP').values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['signalp'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLA binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_HLA-A0201.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_HLA-A0201.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['hla_binding']).values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['hla_a0201'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_HLA-B0702.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_HLA-B0702.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['hla_binding']).values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['hla_b0702'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_H2-Kb.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_H2-Kb.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "df = df.loc[df['sequence'].map(len)<=500]\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['hla_binding']).values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['h2_kb'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SATPDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fns = !ls ./data/satpdb/*.fasta\n",
    "# df = {}\n",
    "# for fn in fns:\n",
    "#     name = fn.split('/')[-1].split('.')[0]\n",
    "#     with open(fn,'r') as f:\n",
    "#         fasta = [l.strip() for l in f]\n",
    "#     df[name] = pd.Series([1]*len(fasta[::2]),index=fasta[1::2],name=name)\n",
    "#     df[name] = df[name].reset_index().drop_duplicates(subset='index',keep='first')\n",
    "#     df[name] = df[name].set_index('index')[name]\n",
    "# df = pd.DataFrame(df).fillna(0)\n",
    "# df.index.name = 'sequence'\n",
    "# df = df.reset_index()\n",
    "# df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "# df = df.loc[df['sequence'].map(len)>=5]\n",
    "# df = df.loc[df['sequence'].map(len)<=100]\n",
    "\n",
    "# for name in df.columns[1:]:\n",
    "#     dataset = NamedTensorDataset(\n",
    "#         sequence=df['sequence'],\n",
    "#         x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "#         x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "#         y=df[name].values[:,None].astype(int)\n",
    "#     )\n",
    "#     datasets[name] = dataset\n",
    "#     print(name,len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "npr.seed(0)\n",
    "\n",
    "MIN_LENGTH = 5\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "for d in datasets:\n",
    "    idxs = [\n",
    "        i \n",
    "        for i,item in enumerate(datasets[d]) \n",
    "        if (len(item['sequence'])>=MIN_LENGTH) and (len(item['sequence'])<=MAX_LENGTH)\n",
    "    ]\n",
    "    npr.shuffle(idxs)\n",
    "    datasets[d] = Subset(datasets[d], idxs)\n",
    "    print(d, len(datasets[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : LinearModel(\n",
    "    output_dim = 1,\n",
    "    model_dim = 128,\n",
    "    num_residues = len(C.alphabet),\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['linear'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : CNNModel(\n",
    "    output_dim = 1,\n",
    "    model_dim = 128,\n",
    "    model_depth = 3,\n",
    "    kernel_size = 3,\n",
    "    num_residues = len(C.alphabet),\n",
    "    dropout = 0.,\n",
    "    lr = 1e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['cnn'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[last_ckpt] = !ls -t1 ./lightning_logs/ms_regularized/checkpoints/*.ckpt | head -n1\n",
    "last_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = 128,\n",
    "    output_dim = 1,\n",
    "    fixed_weights = True,\n",
    "    naive = False,\n",
    "    lr = 1e-4,\n",
    "    output_weights = [(None,1),],\n",
    "    max_length=100\n",
    ")\n",
    "models['ms_pretrained_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lambda : MSModel(\n",
    "#     checkpoint = last_ckpt,\n",
    "#     model_dim = 128,\n",
    "#     output_dim = 1,\n",
    "#     fixed_weights = False,\n",
    "#     naive = False,\n",
    "#     lr = 1e-4,\n",
    "#     output_weights = [(None,1),],\n",
    "#     max_length=100\n",
    "# )\n",
    "# models['ms_pretrained_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = 128,\n",
    "    output_dim = 1,\n",
    "    fixed_weights = False,\n",
    "    naive = True,\n",
    "    lr = 1e-4,\n",
    "    output_weights = [(None,1),],\n",
    "    max_length=100\n",
    ")\n",
    "models['random_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lambda : MSModel(\n",
    "#     checkpoint = last_ckpt,\n",
    "#     model_dim = 128,\n",
    "#     output_dim = 1,\n",
    "#     fixed_weights = False,\n",
    "#     naive = True,\n",
    "#     lr = 1e-4,\n",
    "#     output_weights = [(None,1),],\n",
    "#     max_length=100\n",
    "# )\n",
    "# models['random_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : CARPModel(\n",
    "    output_dim = 1,\n",
    "    fixed_weights = True,\n",
    "    max_length = 100,\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['carp_pretrained_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lambda : CARPModel(\n",
    "#     output_dim = 1,\n",
    "#     fixed_weights = False,\n",
    "#     max_length = 100,\n",
    "#     lr = 1e-4,\n",
    "#     output_weights = [(None,1),]\n",
    "# )\n",
    "# models['carp_pretrained_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to do a proper hparam search for carp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from src.torch_helpers import zero_padding_collate\n",
    "from src.cdhit import cdhit_split\n",
    "\n",
    "seed_everything(0, workers=True)\n",
    "\n",
    "aucs = {}\n",
    "\n",
    "for MODEL in models.keys():\n",
    "    for DATASET in datasets.keys():\n",
    "        name = MODEL+'_'+DATASET\n",
    "        !rm -rf ./lightning_logs/$name\n",
    "# !rm -rf ./lightning_logs/version_$SLURM_JOBID\n",
    "\n",
    "for DATASET in datasets.keys():\n",
    "    dataset = datasets[DATASET]\n",
    "    \n",
    "    sequences = [item['sequence'] for item in dataset]\n",
    "    train_val_seqs, test_seqs, train_val_dataset, test_dataset = cdhit_split(\n",
    "        sequences,\n",
    "        dataset,\n",
    "        split=2./3,\n",
    "        threshold=0.5,\n",
    "        word_length=3\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        collate_fn=zero_padding_collate,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    dm = PeptideDataModule(\n",
    "        train_val_dataset,\n",
    "        batch_size=256,\n",
    "        val_batch_size=-1,\n",
    "        train_val_split=0.5,\n",
    "        cdhit_threshold=0.5,\n",
    "        cdhit_word_length=3,\n",
    "        num_workers=4\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    for MODEL in models.keys():\n",
    "        name = MODEL+'_'+DATASET\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        model = models[MODEL]()\n",
    "        \n",
    "        model.output_weights = [(DATASET,1)]\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            gpus=1,\n",
    "            precision=32,\n",
    "            callbacks=[\n",
    "                NoValProgressBar(),\n",
    "                EarlyStopping(\n",
    "                    monitor=f'val_auc_{DATASET}',\n",
    "                    mode='max',\n",
    "                    patience=10\n",
    "                ),\n",
    "                ModelCheckpoint(\n",
    "                    monitor=f'val_auc_{DATASET}', \n",
    "                    save_top_k=1\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, dm)\n",
    "        \n",
    "        metrics = trainer.test(model, test_dataloader)\n",
    "        \n",
    "        aucs[(DATASET,MODEL)] = metrics[0][f'test_auc_{DATASET}']\n",
    "        \n",
    "        print(DATASET, MODEL)\n",
    "\n",
    "        !mv ./lightning_logs/version_$SLURM_JOBID ./lightning_logs/latest\n",
    "        !mv ./lightning_logs/latest ./lightning_logs/$name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(aucs,index=[0])\n",
    "df.columns.names = ['dataset','model']\n",
    "df = df.T\n",
    "df = df.pivot_table(index='dataset',columns='model')[0]\n",
    "# df['ms_naive'] = [aucs[('cdc28','ms_naive')],aucs[('mito','ms_naive')],aucs[('signalp','ms_naive')]]\n",
    "df.round(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# model = model.cpu()\n",
    "# model.eval()\n",
    "\n",
    "# ys = []\n",
    "# y_preds = []\n",
    "\n",
    "# for batch in dm.val_dataloader():\n",
    "#     y_pred = model.predict_step(batch, 0).detach().cpu().numpy()\n",
    "#     y = batch['y'].cpu().numpy()\n",
    "#     ys.append(y)\n",
    "#     y_preds.append(y_pred)\n",
    "# y = np.concatenate(ys)\n",
    "# y_pred = np.concatenate(y_preds)\n",
    "\n",
    "# for k in range(y.shape[1]):\n",
    "#     plt.figure(figsize=(4,4))\n",
    "#     sns.heatmap(\n",
    "#         confusion_matrix(y[:,k], y_pred[:,k]>0.5),\n",
    "#         annot=True, fmt='d', cmap='Blues'\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MSPretraining]",
   "language": "python",
   "name": "conda-env-.conda-MSPretraining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
