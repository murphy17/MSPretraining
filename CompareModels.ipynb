{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: No xauth data; using fake authentication data for X11 forwarding.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.baselines import CNNModel, CARPModel, MSModel, LinearModel\n",
    "from src.torch_helpers import NamedTensorDataset\n",
    "from src.datamodule import PeptideDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from src.torch_helpers import NoValProgressBar\n",
    "from src.constants import MSConstants\n",
    "C = MSConstants()\n",
    "\n",
    "torch.manual_seed(0);\n",
    "\n",
    "from src.torch_helpers import start_tensorboard\n",
    "start_tensorboard(login_node='login-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate negatives by producing tons of shuffled sequences\n",
    "# then cluster, and only take clusters containing sufficiently many positives\n",
    "\n",
    "# from src.cdhit import CDHIT\n",
    "\n",
    "# def generate_negatives(sequences, num_shuffles=10, min_frac=0.1, random_state=0):\n",
    "#     pos_seqs = list(sequences)\n",
    "#     neg_seqs = []\n",
    "#     rng = npr.RandomState(random_state)\n",
    "#     for n in range(num_shuffles):\n",
    "#         neg_seqs += [''.join(rng.permutation(list(s))) for s in pos_seqs]\n",
    "\n",
    "#     seqs = np.array(pos_seqs + neg_seqs)\n",
    "#     ids = np.array([1]*len(pos_seqs) + [0]*len(neg_seqs))\n",
    "#     clusters = np.array(CDHIT(threshold=0.5,word_length=3).fit_predict(seqs))\n",
    "#     pos_frac = pd.DataFrame([clusters,ids],index=['clusters','ids']).T.groupby('clusters').mean()['ids']\n",
    "#     pos_clusters = set(pos_frac[pos_frac>min_frac].index)\n",
    "    \n",
    "#     pos_seqs = set(pos_seqs)\n",
    "#     negatives = [s for i,s,c in zip(ids,seqs,clusters) if i==0 and c in pos_clusters and s not in pos_seqs]\n",
    "    \n",
    "#     rng.shuffle(negatives)\n",
    "#     negatives = negatives[:len(sequences)]\n",
    "    \n",
    "#     assert len(negatives) == len(sequences)\n",
    "    \n",
    "#     return negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitochondrial targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5349"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/mitochondria_targeting.csv')\n",
    "df = df[['Sequence','Mitochondrial Targeting Signal']].drop_duplicates(keep='first')\n",
    "df = df.loc[df['Sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "# df = df.loc[df['Sequence'].map(len)<=100]\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['Sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df[['Mitochondrial Targeting Signal']].astype(np.int32).values\n",
    ")\n",
    "\n",
    "datasets['mito'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cdc28 binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/cdc28_binding.csv')\n",
    "df = df[['Sequence','Cdc28 Binding']].drop_duplicates(keep='first')\n",
    "df = df.loc[df['Sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "# df = df.loc[df['Sequence'].map(len)<=100]\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['Sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df[['Cdc28 Binding']].astype(np.int32).values\n",
    ")\n",
    "\n",
    "datasets['cdc28'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20290"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_set.fasta','r') as f:\n",
    "    fasta = [l.strip() for l in f]\n",
    "    df = pd.Series(fasta[::3]).str.extract(\n",
    "        '>(?P<uniprot>[^\\|]+)\\|(?P<kingdom>[^|]+)\\|(?P<type>[^|]+)\\|(?P<partition>[^|]+)'\n",
    "    )\n",
    "    df['sequence'] = fasta[1::3]\n",
    "    df['annotation'] = fasta[2::3]\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "# df = df.loc[df['sequence'].map(len)<=100]\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['type']!='NO_SP').values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['signalp'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLA binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11747"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_HLA-A0201.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_HLA-A0201.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['hla_binding']).values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['hla_a0201'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3627"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_HLA-B0702.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_HLA-B0702.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['hla_binding']).values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['hla_b0702'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2598"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_H2-Kb.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_H2-Kb.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=(df['hla_binding']).values[:,None].astype(int)\n",
    ")\n",
    "\n",
    "datasets['h2_kb'] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SATPDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fns = !ls ./data/satpdb/*.fasta\n",
    "# df = {}\n",
    "# for fn in fns:\n",
    "#     name = fn.split('/')[-1].split('.')[0]\n",
    "#     with open(fn,'r') as f:\n",
    "#         fasta = [l.strip() for l in f]\n",
    "#     df[name] = pd.Series([1]*len(fasta[::2]),index=fasta[1::2],name=name)\n",
    "#     df[name] = df[name].reset_index().drop_duplicates(subset='index',keep='first')\n",
    "#     df[name] = df[name].set_index('index')[name]\n",
    "# df = pd.DataFrame(df).fillna(0)\n",
    "# df.index.name = 'sequence'\n",
    "# df = df.reset_index()\n",
    "# df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "# df = df.loc[df['sequence'].map(len)>=5]\n",
    "# df = df.loc[df['sequence'].map(len)<=100]\n",
    "\n",
    "# for name in df.columns[1:]:\n",
    "#     dataset = NamedTensorDataset(\n",
    "#         sequence=df['sequence'],\n",
    "#         x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "#         x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "#         y=df[name].values[:,None].astype(int)\n",
    "#     )\n",
    "#     datasets[name] = dataset\n",
    "#     print(name,len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : LinearModel(\n",
    "    output_dim = 1,\n",
    "    model_dim = 128,\n",
    "    num_residues = len(C.alphabet),\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['linear'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : CNNModel(\n",
    "    output_dim = 1,\n",
    "    model_dim = 128,\n",
    "    model_depth = 3,\n",
    "    kernel_size = 3,\n",
    "    num_residues = len(C.alphabet),\n",
    "    dropout = 0.1,\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['cnn'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "[last_ckpt] = !ls -t1 ./ms_carp/checkpoints/*.ckpt | head -n1\n",
    "model = lambda : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = 128,\n",
    "    output_dim = 1,\n",
    "    fixed_weights = True,\n",
    "    naive = False,\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),],\n",
    "    max_length=100\n",
    ")\n",
    "models['ms_pretrained_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "[last_ckpt] = !ls -t1 ./ms_carp/checkpoints/*.ckpt | head -n1\n",
    "model = lambda : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = 128,\n",
    "    output_dim = 1,\n",
    "    fixed_weights = False,\n",
    "    naive = False,\n",
    "    lr = 1e-4,\n",
    "    output_weights = [(None,1),],\n",
    "    max_length=100\n",
    ")\n",
    "models['ms_pretrained_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "[last_ckpt] = !ls -t1 ./ms_carp/checkpoints/*.ckpt | head -n1\n",
    "model = lambda : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = 128,\n",
    "    output_dim = 1,\n",
    "    fixed_weights = False,\n",
    "    naive = True,\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),],\n",
    "    max_length=100\n",
    ")\n",
    "models['random_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "[last_ckpt] = !ls -t1 ./ms_carp/checkpoints/*.ckpt | head -n1\n",
    "model = lambda : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = 128,\n",
    "    output_dim = 1,\n",
    "    fixed_weights = False,\n",
    "    naive = True,\n",
    "    lr = 1e-4,\n",
    "    output_weights = [(None,1),],\n",
    "    max_length=100\n",
    ")\n",
    "models['random_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : CARPModel(\n",
    "    output_dim = 1,\n",
    "    fixed_weights = True,\n",
    "    max_length = 100,\n",
    "    lr = 5e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['carp_pretrained_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda : CARPModel(\n",
    "    output_dim = 1,\n",
    "    fixed_weights = False,\n",
    "    max_length = 100,\n",
    "    lr = 1e-4,\n",
    "    output_weights = [(None,1),]\n",
    ")\n",
    "models['carp_pretrained_finetune'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./lightning_logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | embedding  | Embedding | 3.1 K \n",
      "1 | classifier | Linear    | 129   \n",
      "-----------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1938: PossibleUserWarning: The number of training samples (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163: 100%|██████████| 7/7 [02:26<00:00, 20.87s/it, loss=0.135, v_num=1.79e+7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.8187252283096313\n",
      "        test_loss           0.12637165188789368\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | embedding  | Embedding  | 3.1 K \n",
      "1 | encoder    | Sequential | 31.0 K\n",
      "2 | classifier | Linear     | 33    \n",
      "------------------------------------------\n",
      "34.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "34.1 K    Total params\n",
      "0.136     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 7/7 [00:46<00:00,  6.65s/it, loss=0.185, v_num=1.79e+7]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.5752437114715576\n",
      "        test_loss           0.17195670306682587\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | transformer | MSTransformer  | 655 K \n",
      "1 | classifier  | ESMAttention1d | 16.8 K\n",
      "-----------------------------------------------\n",
      "16.8 K    Trainable params\n",
      "655 K     Non-trainable params\n",
      "672 K     Total params\n",
      "2.690     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 7/7 [15:24<00:00, 132.12s/it, loss=0.105, v_num=1.79e+7]  \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:11<00:00, 11.13s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.8757753372192383\n",
      "        test_loss           0.10595064610242844\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito ms_pretrained_frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | transformer | MSTransformer  | 655 K \n",
      "1 | classifier  | ESMAttention1d | 16.8 K\n",
      "-----------------------------------------------\n",
      "672 K     Trainable params\n",
      "0         Non-trainable params\n",
      "672 K     Total params\n",
      "2.690     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [02:18<00:00, 19.77s/it, loss=nan, v_num=1.79e+7]  \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:13<00:00, 13.60s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.5279594659805298\n",
      "        test_loss                   nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito random_frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | transformer | MSTransformer  | 655 K \n",
      "1 | classifier  | ESMAttention1d | 16.8 K\n",
      "-----------------------------------------------\n",
      "672 K     Trainable params\n",
      "0         Non-trainable params\n",
      "672 K     Total params\n",
      "2.690     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [02:35<00:00, 22.22s/it, loss=nan, v_num=1.79e+7]  \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:13<00:00, 13.52s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.5279594659805298\n",
      "        test_loss                   nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito ms_pretrained_finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | transformer | MSTransformer  | 655 K \n",
      "1 | classifier  | ESMAttention1d | 16.8 K\n",
      "-----------------------------------------------\n",
      "672 K     Trainable params\n",
      "0         Non-trainable params\n",
      "672 K     Total params\n",
      "2.690     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [02:20<00:00, 20.02s/it, loss=nan, v_num=1.79e+7] \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:10<00:00, 10.68s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.5279594659805298\n",
      "        test_loss                   nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito random_finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | encoder    | ByteNet        | 603 K \n",
      "1 | classifier | ESMAttention1d | 16.8 K\n",
      "----------------------------------------------\n",
      "16.8 K    Trainable params\n",
      "603 K     Non-trainable params\n",
      "620 K     Total params\n",
      "2.481     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 7/7 [11:44<00:00, 100.61s/it, loss=0.1, v_num=1.79e+7]     \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:12<00:00, 12.99s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.8286514282226562\n",
      "        test_loss           0.11465723812580109\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito carp_pretrained_frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | encoder    | ByteNet        | 603 K \n",
      "1 | classifier | ESMAttention1d | 16.8 K\n",
      "----------------------------------------------\n",
      "620 K     Trainable params\n",
      "0         Non-trainable params\n",
      "620 K     Total params\n",
      "2.481     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [02:20<00:00, 20.08s/it, loss=nan, v_num=1.79e+7]  \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:13<00:00, 13.19s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.5279594659805298\n",
      "        test_loss                   nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito carp_pretrained_finetune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | embedding  | Embedding | 3.1 K \n",
      "1 | classifier | Linear    | 129   \n",
      "-----------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:04<00:00,  1.52it/s, loss=0.597, v_num=1.79e+7]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_auc_cdc28         0.5056790113449097\n",
      "        test_loss           0.5294088125228882\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "cdc28 linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | embedding  | Embedding  | 3.1 K \n",
      "1 | encoder    | Sequential | 31.0 K\n",
      "2 | classifier | Linear     | 33    \n",
      "------------------------------------------\n",
      "34.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "34.1 K    Total params\n",
      "0.136     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 7/7 [01:10<00:00, 10.05s/it, loss=0.164, v_num=1.79e+7]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_auc_cdc28         0.8263400793075562\n",
      "        test_loss           0.1368022859096527\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "cdc28 cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | transformer | MSTransformer  | 655 K \n",
      "1 | classifier  | ESMAttention1d | 16.8 K\n",
      "-----------------------------------------------\n",
      "16.8 K    Trainable params\n",
      "655 K     Non-trainable params\n",
      "672 K     Total params\n",
      "2.690     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  71%|███████▏  | 5/7 [07:29<02:59, 89.87s/it, loss=0.153, v_num=1.79e+7] "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from src.torch_helpers import zero_padding_collate\n",
    "from src.cdhit import cdhit_split\n",
    "\n",
    "aucs = {}\n",
    "\n",
    "for DATASET in datasets.keys():\n",
    "    dataset = datasets[DATASET]\n",
    "    \n",
    "    sequences = [item['sequence'] for item in dataset]\n",
    "    train_val_seqs, test_seqs, train_val_dataset, test_dataset = cdhit_split(\n",
    "        sequences,\n",
    "        dataset,\n",
    "        split=2./3,\n",
    "        threshold=0.5,\n",
    "        word_length=3\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        collate_fn=zero_padding_collate,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    dm = PeptideDataModule(\n",
    "        train_val_dataset,\n",
    "        batch_size=256,\n",
    "        val_batch_size=-1,\n",
    "        train_val_split=0.5,\n",
    "        cdhit_threshold=0.5,\n",
    "        cdhit_word_length=3,\n",
    "        num_workers=4\n",
    "    )\n",
    "    dm.setup()\n",
    "\n",
    "    for MODEL in models.keys():\n",
    "        name = MODEL+'_'+DATASET\n",
    "        !rm -rf ./lightning_logs/$name\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        model = models[MODEL]()\n",
    "        \n",
    "        model.output_weights = [(DATASET,1)]\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            gpus=0,\n",
    "            precision=32,\n",
    "            callbacks=[\n",
    "                NoValProgressBar(),\n",
    "                EarlyStopping(\n",
    "                    monitor=f'val_auc_{DATASET}',\n",
    "                    mode='max',\n",
    "                    patience=3\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, dm)\n",
    "        \n",
    "        metrics = trainer.test(model, test_dataloader)\n",
    "        \n",
    "        aucs[(DATASET,MODEL)] = metrics[0][f'test_auc_{DATASET}']\n",
    "        \n",
    "        print(DATASET, MODEL)\n",
    "\n",
    "        !mv ./lightning_logs/version_$SLURM_JOBID ./lightning_logs/latest\n",
    "        !mv ./lightning_logs/latest ./lightning_logs/$name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(aucs,index=[0])\n",
    "df.columns.names = ['dataset','model']\n",
    "df = df.T\n",
    "df = df.pivot_table(index='dataset',columns='model')[0]\n",
    "# df['ms_naive'] = [aucs[('cdc28','ms_naive')],aucs[('mito','ms_naive')],aucs[('signalp','ms_naive')]]\n",
    "df.round(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# model = model.cpu()\n",
    "# model.eval()\n",
    "\n",
    "# ys = []\n",
    "# y_preds = []\n",
    "\n",
    "# for batch in dm.val_dataloader():\n",
    "#     y_pred = model.predict_step(batch, 0).detach().cpu().numpy()\n",
    "#     y = batch['y'].cpu().numpy()\n",
    "#     ys.append(y)\n",
    "#     y_preds.append(y_pred)\n",
    "# y = np.concatenate(ys)\n",
    "# y_pred = np.concatenate(y_preds)\n",
    "\n",
    "# for k in range(y.shape[1]):\n",
    "#     plt.figure(figsize=(4,4))\n",
    "#     sns.heatmap(\n",
    "#         confusion_matrix(y[:,k], y_pred[:,k]>0.5),\n",
    "#         annot=True, fmt='d', cmap='Blues'\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MSPretraining]",
   "language": "python",
   "name": "conda-env-.conda-MSPretraining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
