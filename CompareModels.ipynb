{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.baselines import CNNModel, CARPModel, MSModel, LinearModel\n",
    "from src.torch_helpers import NamedTensorDataset\n",
    "from src.datamodule import PeptideDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from src.torch_helpers import NoValProgressBar\n",
    "from src.constants import MSConstants\n",
    "C = MSConstants()\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.torch_helpers import start_tensorboard\n",
    "# start_tensorboard(login_node='login-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate negatives by producing tons of shuffled sequences\n",
    "# then cluster, and only take clusters containing sufficiently many positives\n",
    "\n",
    "# from src.cdhit import CDHIT\n",
    "\n",
    "# def generate_negatives(sequences, num_shuffles=10, min_frac=0.1, random_state=0):\n",
    "#     pos_seqs = list(sequences)\n",
    "#     neg_seqs = []\n",
    "#     rng = npr.RandomState(random_state)\n",
    "#     for n in range(num_shuffles):\n",
    "#         neg_seqs += [''.join(rng.permutation(list(s))) for s in pos_seqs]\n",
    "\n",
    "#     seqs = np.array(pos_seqs + neg_seqs)\n",
    "#     ids = np.array([1]*len(pos_seqs) + [0]*len(neg_seqs))\n",
    "#     clusters = np.array(CDHIT(threshold=0.5,word_length=3).fit_predict(seqs))\n",
    "#     pos_frac = pd.DataFrame([clusters,ids],index=['clusters','ids']).T.groupby('clusters').mean()['ids']\n",
    "#     pos_clusters = set(pos_frac[pos_frac>min_frac].index)\n",
    "    \n",
    "#     pos_seqs = set(pos_seqs)\n",
    "#     negatives = [s for i,s,c in zip(ids,seqs,clusters) if i==0 and c in pos_clusters and s not in pos_seqs]\n",
    "    \n",
    "#     rng.shuffle(negatives)\n",
    "#     negatives = negatives[:len(sequences)]\n",
    "    \n",
    "#     assert len(negatives) == len(sequences)\n",
    "    \n",
    "#     return negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitochondrial targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5349"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/mitochondria_targeting.csv')\n",
    "df = df[['Sequence','Mitochondrial Targeting Signal']].drop_duplicates(keep='first')\n",
    "df = df.loc[df['Sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    'mito',\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['Sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df['Mitochondrial Targeting Signal'].astype(int)\n",
    ")\n",
    "\n",
    "datasets[dataset.name] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cdc28 binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/cdc28_binding.csv')\n",
    "df = df[['Sequence','Cdc28 Binding']].drop_duplicates(keep='first')\n",
    "df = df.loc[df['Sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    'cdc28',\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['Sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df['Cdc28 Binding'].astype(int)\n",
    ")\n",
    "\n",
    "datasets[dataset.name] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal peptide - MUST MULTITASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20290"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/train_set.fasta','r') as f:\n",
    "    fasta = [l.strip() for l in f]\n",
    "    df = pd.Series(fasta[::3]).str.extract(\n",
    "        '>(?P<uniprot>[^\\|]+)\\|(?P<kingdom>[^|]+)\\|(?P<type>[^|]+)\\|(?P<partition>[^|]+)'\n",
    "    )\n",
    "    df['sequence'] = fasta[1::3]\n",
    "    df['annotation'] = fasta[2::3]\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    'signalp',\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df['type'].astype('category').cat.codes\n",
    ")\n",
    "\n",
    "datasets[dataset.name] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLA binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11747"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_seqs = !cat ./data/mlci2012/binding_HLA-A0201.txt\n",
    "neg_seqs = !cat ./data/mlci2012/nonbinding_HLA-A0201.txt\n",
    "df = pd.DataFrame({\n",
    "    'sequence': pos_seqs + neg_seqs,\n",
    "    'hla_binding': [1]*len(pos_seqs) + [0]*len(neg_seqs)\n",
    "})\n",
    "df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "df = df.sample(frac=1.,random_state=0)\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    'hla',\n",
    "    sequence=df['sequence'],\n",
    "    x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "    x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "    y=df['hla_binding'].astype(int)\n",
    ")\n",
    "\n",
    "datasets[dataset.name] = dataset\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SATPDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fns = !ls ./data/satpdb/*.fasta\n",
    "# df = {}\n",
    "# for fn in fns:\n",
    "#     name = fn.split('/')[-1].split('.')[0]\n",
    "#     with open(fn,'r') as f:\n",
    "#         fasta = [l.strip() for l in f]\n",
    "#     df[name] = pd.Series([1]*len(fasta[::2]),index=fasta[1::2],name=name)\n",
    "#     df[name] = df[name].reset_index().drop_duplicates(subset='index',keep='first')\n",
    "#     df[name] = df[name].set_index('index')[name]\n",
    "# df = pd.DataFrame(df).fillna(0)\n",
    "# df.index.name = 'sequence'\n",
    "# df = df.reset_index()\n",
    "# df = df.loc[df['sequence'].map(lambda s: all([c==c.upper() and c in C.alphabet for c in s]))]\n",
    "# df = df.loc[df['sequence'].map(len)>=5]\n",
    "# df = df.loc[df['sequence'].map(len)<=100]\n",
    "\n",
    "# for name in df.columns[1:]:\n",
    "#     dataset = NamedTensorDataset(\n",
    "#         sequence=df['sequence'],\n",
    "#         x=df['sequence'].map(lambda s: [C.alphabet.index(c) for c in s]),\n",
    "#         x_mask=df['sequence'].map(lambda s: [1]*len(s)),\n",
    "#         y=df[name].values[:,None].astype(int)\n",
    "#     )\n",
    "#     datasets[name] = dataset\n",
    "#     print(name,len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mito 4121\n",
      "cdc28 4120\n",
      "signalp 20290\n",
      "hla 11747\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "npr.seed(0)\n",
    "\n",
    "MIN_LENGTH = 5\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "for d in datasets:\n",
    "    idxs = [\n",
    "        i \n",
    "        for i,item in enumerate(datasets[d]) \n",
    "        if (len(item['sequence'])>=MIN_LENGTH) and (len(item['sequence'])<=MAX_LENGTH)\n",
    "    ]\n",
    "    npr.shuffle(idxs)\n",
    "    datasets[d] = Subset(datasets[d], idxs)\n",
    "    print(d, len(datasets[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = 128\n",
    "LR = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./version_17869980/checkpoints/epoch=92-step=114948-best.ckpt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[last_ckpt] = !ls -t1 ./version_17869980/checkpoints/*.ckpt | head -n1\n",
    "last_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda output_dim: MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = MODEL_DIM,\n",
    "    output_dim = output_dim,\n",
    "    fixed_weights = True,\n",
    "    naive = False,\n",
    "    lr = LR\n",
    ")\n",
    "models['ms_pretrained_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lambda output_dim: MSModel(\n",
    "#     checkpoint = last_ckpt,\n",
    "#     model_dim = MODEL_DIM,\n",
    "#     output_dim = output_dim,\n",
    "#     fixed_weights = False,\n",
    "#     naive = False,\n",
    "#     lr = LR\n",
    "# )\n",
    "# models['ms_pretrained_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda output_dim: LinearModel(\n",
    "    output_dim = output_dim,\n",
    "    model_dim = MODEL_DIM,\n",
    "    num_residues = len(C.alphabet),\n",
    "    lr = LR,\n",
    ")\n",
    "models['linear'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2011.03443.pdf\n",
    "# differences: 128 vs 1024, attention vs max pool\n",
    "model = lambda output_dim : CNNModel(\n",
    "    output_dim = output_dim,\n",
    "    model_dim = MODEL_DIM,\n",
    "    model_depth = 3,\n",
    "    kernel_size = 5,\n",
    "    num_residues = len(C.alphabet),\n",
    "    dropout = 0.,\n",
    "    lr = LR,\n",
    ")\n",
    "models['cnn'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda output_dim : MSModel(\n",
    "    checkpoint = last_ckpt,\n",
    "    model_dim = MODEL_DIM,\n",
    "    output_dim = output_dim,\n",
    "    fixed_weights = False,\n",
    "    naive = True,\n",
    "    lr = LR,\n",
    ")\n",
    "models['random_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lambda output_dim : MSModel(\n",
    "#     checkpoint = last_ckpt,\n",
    "#     model_dim = MODEL_DIM,\n",
    "#     output_dim = output_dim,\n",
    "#     fixed_weights = True,\n",
    "#     naive = True,\n",
    "#     lr = LR\n",
    "# )\n",
    "# models['random_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda output_dim : CARPModel(\n",
    "    output_dim = output_dim,\n",
    "    fixed_weights = True,\n",
    "    lr = LR,\n",
    ")\n",
    "models['carp_pretrained_frozen'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = lambda output_dim : CARPModel(\n",
    "#     output_dim = output_dim,\n",
    "#     fixed_weights = False,\n",
    "#     lr = LR,\n",
    "# )\n",
    "# models['carp_pretrained_finetune'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# you cannot do this on CPU! it gives NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for MODEL in models.keys():\n",
    "    for DATASET in datasets.keys():\n",
    "        name = MODEL+'_'+DATASET\n",
    "        !rm -rf ./lightning_logs/$name\n",
    "!rm -rf ./lightning_logs/version_$SLURM_JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | encoder    | ByteNet        | 603 K \n",
      "1 | classifier | ESMAttention1d | 16.8 K\n",
      "----------------------------------------------\n",
      "16.8 K    Trainable params\n",
      "603 K     Non-trainable params\n",
      "620 K     Total params\n",
      "2.481     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1938: PossibleUserWarning: The number of training samples (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 6/6 [02:01<00:00, 20.27s/it, loss=0.0515, v_num=1.79e+7] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.8738208413124084\n",
      "        test_loss           0.11530359089374542\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito ms_pretrained_frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | embedding  | Embedding | 3.1 K \n",
      "1 | classifier | Linear    | 129   \n",
      "-----------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 6/6 [01:57<00:00, 19.65s/it, loss=0.132, v_num=1.79e+7] \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.8317251205444336\n",
      "        test_loss           0.12419627606868744\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | embedding  | Embedding  | 3.1 K \n",
      "1 | encoder    | Sequential | 51.5 K\n",
      "2 | classifier | Linear     | 33    \n",
      "------------------------------------------\n",
      "54.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "54.6 K    Total params\n",
      "0.218     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 6/6 [00:27<00:00,  4.59s/it, loss=0.411, v_num=1.79e+7]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.7852209806442261\n",
      "        test_loss           0.4843822121620178\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | encoder    | ByteNet        | 603 K \n",
      "1 | classifier | ESMAttention1d | 16.8 K\n",
      "----------------------------------------------\n",
      "620 K     Trainable params\n",
      "0         Non-trainable params\n",
      "620 K     Total params\n",
      "2.481     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 6/6 [00:47<00:00,  7.95s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 6/6 [00:52<00:00,  8.71s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 6/6 [00:57<00:00,  9.51s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 6/6 [01:01<00:00, 10.25s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 6/6 [01:05<00:00, 10.99s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 6/6 [01:10<00:00, 11.73s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 6/6 [01:15<00:00, 12.53s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 6/6 [01:19<00:00, 13.22s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 6/6 [01:23<00:00, 13.99s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 6/6 [01:28<00:00, 14.72s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 6/6 [01:33<00:00, 15.50s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 6/6 [01:37<00:00, 16.24s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 6/6 [01:41<00:00, 16.92s/it, loss=nan, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 6/6 [01:45<00:00, 17.66s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 6/6 [01:50<00:00, 18.44s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 6/6 [01:54<00:00, 19.14s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 6/6 [01:59<00:00, 19.92s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 6/6 [02:04<00:00, 20.67s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 6/6 [02:08<00:00, 21.41s/it, loss=nan, v_num=1.79e+7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (30) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 6/6 [02:12<00:00, 22.16s/it, loss=nan, v_num=1.79e+7] \n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_auc_mito         0.5863801836967468\n",
      "        test_loss                   nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "mito random_frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | encoder    | ByteNet        | 603 K \n",
      "1 | classifier | ESMAttention1d | 16.8 K\n",
      "----------------------------------------------\n",
      "16.8 K    Trainable params\n",
      "603 K     Non-trainable params\n",
      "620 K     Total params\n",
      "2.481     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  67%|██████▋   | 4/6 [00:12<00:06,  3.16s/it, loss=0.151, v_num=1.79e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_ckpt)\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 88\u001b[0m metrics, \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(model, test_dataloader)\n\u001b[1;32m     90\u001b[0m aucs[(DATASET,MODEL)] \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_auc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(DATASET, MODEL)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from src.torch_helpers import zero_padding_collate\n",
    "from src.cdhit import cdhit_split\n",
    "\n",
    "seed_everything(0, workers=True)\n",
    "\n",
    "aucs = {}\n",
    "\n",
    "for MODEL in models.keys():\n",
    "    for DATASET in datasets.keys():\n",
    "        name = MODEL+'_'+DATASET\n",
    "        !rm -rf ./lightning_logs/$name\n",
    "!rm -rf ./lightning_logs/version_$SLURM_JOBID\n",
    "\n",
    "for DATASET in datasets.keys():\n",
    "    dataset = datasets[DATASET]\n",
    "    \n",
    "    sequences = [item['sequence'] for item in dataset]\n",
    "    train_val_seqs, test_seqs, train_val_dataset, test_dataset = cdhit_split(\n",
    "        sequences,\n",
    "        dataset,\n",
    "        split=2./3,\n",
    "        threshold=0.5,\n",
    "        word_length=3\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        collate_fn=zero_padding_collate,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    dm = PeptideDataModule(\n",
    "        train_val_dataset,\n",
    "        batch_size=256,\n",
    "        val_batch_size=-1,\n",
    "        train_val_split=0.5,\n",
    "        cdhit_threshold=0.5,\n",
    "        cdhit_word_length=3,\n",
    "        num_workers=4\n",
    "    )\n",
    "    dm.setup()\n",
    "    \n",
    "    OUTPUT_DIM = 6 if DATASET == 'signalp' else 1\n",
    "\n",
    "    for MODEL in models.keys():\n",
    "        name = MODEL+'_'+DATASET\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        model = models[MODEL](OUTPUT_DIM)\n",
    "        \n",
    "        model.name = DATASET\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            gpus=0,\n",
    "            precision=32,\n",
    "            max_epochs=1000,\n",
    "            min_epochs=30,\n",
    "            callbacks=[\n",
    "                NoValProgressBar(),\n",
    "                EarlyStopping(\n",
    "                    monitor=f'val_auc_{DATASET}',\n",
    "                    mode='max',\n",
    "                    patience=10\n",
    "                ),\n",
    "                ModelCheckpoint(\n",
    "                    monitor=f'val_auc_{DATASET}', \n",
    "                    mode='max',\n",
    "                    save_top_k=1,\n",
    "                    filename='{epoch}-{step}-best'\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, dm)\n",
    "        \n",
    "        [best_ckpt] = !ls -t1 ./lightning_logs/version_$SLURM_JOBID/checkpoints/*.ckpt | head -n1\n",
    "        checkpoint = torch.load(best_ckpt)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        metrics, = trainer.test(model, test_dataloader)\n",
    "        \n",
    "        aucs[(DATASET,MODEL)] = metrics[f'test_auc_{DATASET}']\n",
    "        \n",
    "        print(DATASET, MODEL)\n",
    "\n",
    "        !mv ./lightning_logs/version_$SLURM_JOBID ./lightning_logs/latest\n",
    "        !mv ./lightning_logs/latest ./lightning_logs/$name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(aucs,index=[0])\n",
    "df.columns.names = ['dataset','model']\n",
    "df = df.T\n",
    "df = df.pivot_table(index='dataset',columns='model')[0]\n",
    "# df['ms_naive'] = [aucs[('cdc28','ms_naive')],aucs[('mito','ms_naive')],aucs[('signalp','ms_naive')]]\n",
    "df.round(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# model = model.cpu()\n",
    "# model.eval()\n",
    "\n",
    "# ys = []\n",
    "# y_preds = []\n",
    "\n",
    "# for batch in dm.val_dataloader():\n",
    "#     y_pred = model.predict_step(batch, 0).detach().cpu().numpy()\n",
    "#     y = batch['y'].cpu().numpy()\n",
    "#     ys.append(y)\n",
    "#     y_preds.append(y_pred)\n",
    "# y = np.concatenate(ys)\n",
    "# y_pred = np.concatenate(y_preds)\n",
    "\n",
    "# for k in range(y.shape[1]):\n",
    "#     plt.figure(figsize=(4,4))\n",
    "#     sns.heatmap(\n",
    "#         confusion_matrix(y[:,k], y_pred[:,k]>0.5),\n",
    "#         annot=True, fmt='d', cmap='Blues'\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MSPretraining]",
   "language": "python",
   "name": "conda-env-.conda-MSPretraining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
