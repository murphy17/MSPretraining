{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Mitochondrial Targeting Signal</th>\n",
       "      <th>Cdc28 Binding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MPAVLRTRSKESSIEQKPASRTRTRSRRGKRGRDDDDDDDDEESDD...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EQKWQDEQELKKKEKELKRKNDAEAKRLRMEERKRQQMQKKIAKEQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IEKFKTKKIKAKLKADQKLNKEDAKPGSDVEKEVSFNPLF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MQKISKYSSMAILRKRPLVKTETGPESELLPEKRTKIKQEEVVPQPVD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RELNVEAEINVKHEEKTVEETMVKLENDISVKVED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>MSDYEEAFNDGNENFEDFDVEHFSDEETYEEKPQFKDGETTDANGK...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>PPEGHKKTEKETDIKDVDETNEDEVKDRVEDEVKDRVEDEVKDQDE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>MDELLGEALSAENQTGESTVESEKLVTPEDVMTIS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5347</th>\n",
       "      <td>PLSDLKKRSQAKMNAKTDFAKIINKPNELSQILTVDPKT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5348</th>\n",
       "      <td>DNKKRSGSNAAASLPSKKLKTEDGFVIPALPAAVSKSLQESGDTQE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sequence  \\\n",
       "0     MPAVLRTRSKESSIEQKPASRTRTRSRRGKRGRDDDDDDDDEESDD...   \n",
       "1     EQKWQDEQELKKKEKELKRKNDAEAKRLRMEERKRQQMQKKIAKEQ...   \n",
       "2              IEKFKTKKIKAKLKADQKLNKEDAKPGSDVEKEVSFNPLF   \n",
       "3      MQKISKYSSMAILRKRPLVKTETGPESELLPEKRTKIKQEEVVPQPVD   \n",
       "4                   RELNVEAEINVKHEEKTVEETMVKLENDISVKVED   \n",
       "...                                                 ...   \n",
       "5344  MSDYEEAFNDGNENFEDFDVEHFSDEETYEEKPQFKDGETTDANGK...   \n",
       "5345  PPEGHKKTEKETDIKDVDETNEDEVKDRVEDEVKDRVEDEVKDQDE...   \n",
       "5346                MDELLGEALSAENQTGESTVESEKLVTPEDVMTIS   \n",
       "5347            PLSDLKKRSQAKMNAKTDFAKIINKPNELSQILTVDPKT   \n",
       "5348  DNKKRSGSNAAASLPSKKLKTEDGFVIPALPAAVSKSLQESGDTQE...   \n",
       "\n",
       "      Mitochondrial Targeting Signal  Cdc28 Binding  \n",
       "0                                  1              0  \n",
       "1                                  0              0  \n",
       "2                                  0              0  \n",
       "3                                  0              0  \n",
       "4                                  0              0  \n",
       "...                              ...            ...  \n",
       "5344                               0              0  \n",
       "5345                               0              0  \n",
       "5346                               0              0  \n",
       "5347                               0              0  \n",
       "5348                               0              0  \n",
       "\n",
       "[5340 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mito_df = pd.read_csv('./data/mitochondria_targeting.csv')\n",
    "mito_df = mito_df[['Sequence','Mitochondrial Targeting Signal']].drop_duplicates(keep='first')\n",
    "cdc28_df = pd.read_csv('./data/cdc28_binding.csv')\n",
    "cdc28_df = cdc28_df[['Sequence','Cdc28 Binding']].drop_duplicates(keep='first')\n",
    "# seem to be same sequences.\n",
    "df = mito_df.merge(cdc28_df,on='Sequence',how='inner')\n",
    "# df = df.loc[df['Sequence'].map(len)<=50] # per the review from Kevin\n",
    "df = df.loc[df['Sequence'].map(len)<=1000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.torch_helpers import NamedTensorDataset\n",
    "from src.datamodule import PeptideDataModule\n",
    "from src.constants import MSConstants\n",
    "\n",
    "C = MSConstants()\n",
    "\n",
    "dataset = NamedTensorDataset(\n",
    "    sequence=df['Sequence'],\n",
    "    x=df['Sequence'].map(lambda s: np.array([C.alphabet.index(c) for c in s])),\n",
    "    x_mask=df['Sequence'].map(lambda s: np.array([1 for c in s])),\n",
    "#     y=df[['Mitochondrial Targeting Signal','Cdc28 Binding']].astype(np.int32).values\n",
    "    y=df[['Mitochondrial Targeting Signal']].astype(np.int32).values\n",
    ")\n",
    "\n",
    "dm = PeptideDataModule(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    val_batch_size=1024,\n",
    "    train_val_split=0.9,\n",
    "    cdhit_threshold=0.5,\n",
    "    cdhit_word_length=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import auroc\n",
    "\n",
    "class CNNModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        model_dim,\n",
    "        model_depth,\n",
    "        kernel_size,\n",
    "        num_residues,\n",
    "        dropout,\n",
    "        balance_classes,\n",
    "        lr\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_dim = model_dim\n",
    "        self.model_depth = model_depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_residues = num_residues\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.balance_classes = balance_classes\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_residues,\n",
    "            embedding_dim=model_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        encoder_layers = []\n",
    "        in_dim = model_dim\n",
    "        for i in range(model_depth-1):\n",
    "            out_dim = in_dim // 2\n",
    "            drop = nn.Dropout(dropout)\n",
    "            conv = nn.Conv1d(in_dim,out_dim,kernel_size,padding=kernel_size//2)\n",
    "            norm = nn.BatchNorm1d(out_dim)\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "            pool = nn.AvgPool1d(2,2)\n",
    "            encoder_layers += [drop, conv, norm, relu, pool]\n",
    "            in_dim = in_dim // 2\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.classifier = nn.Linear(out_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.swapdims(1,2)\n",
    "        x = self.encoder(x)\n",
    "        x = self.pooling(x).squeeze(-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(),lr=self.lr)\n",
    "        return opt\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_pred = self(x)\n",
    "        losses = []\n",
    "        aucs = []\n",
    "        for k in range(self.output_dim):\n",
    "            if self.balance_classes:\n",
    "                pos_weight = (1+(y[:,k]==0).sum()) / (1+(y[:,k]==1).sum())\n",
    "            else:\n",
    "                pos_weight = None\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred[:,k], y[:,k].float(), pos_weight=pos_weight)\n",
    "            auc = auroc(y_pred[:,k], y[:,k])\n",
    "            losses.append(loss)\n",
    "            aucs.append(auc)\n",
    "        loss = torch.stack(losses).mean()\n",
    "        return loss, aucs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch_size = batch['x'].shape[0]\n",
    "        loss, aucs = self.step(batch, batch_idx)\n",
    "        self.log('train_loss',loss,batch_size=batch_size)\n",
    "        for k in range(self.output_dim):\n",
    "            self.log(f'train_auc_{k}',aucs[k],batch_size=batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch_size = batch['x'].shape[0]\n",
    "        loss, aucs = self.step(batch, batch_idx)\n",
    "        self.log('valid_loss',loss,sync_dist=True,batch_size=batch_size)\n",
    "        for k in range(self.output_dim):\n",
    "            self.log(f'valid_auc_{k}',aucs[k],sync_dist=True,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = CNNModel(\n",
    "    output_dim = len(dm.dataset[0]['y']),\n",
    "    model_dim = 64,\n",
    "    model_depth = 3,\n",
    "    num_residues = len(C.alphabet),\n",
    "    kernel_size = 3,\n",
    "    dropout = 0.1,\n",
    "    lr = 5e-4,\n",
    "    balance_classes = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./lightning_logs/version_$SLURM_JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | embedding  | Embedding         | 1.5 K \n",
      "1 | encoder    | Sequential        | 7.8 K \n",
      "2 | pooling    | AdaptiveAvgPool1d | 0     \n",
      "3 | classifier | Linear            | 17    \n",
      "-------------------------------------------------\n",
      "9.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.4 K     Total params\n",
      "0.038     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  58%|█████▊    | 11/19 [00:01<00:00, 10.23it/s, loss=1.06, v_num=1.66e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/mmurphy/.conda/envs/MSPretraining/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 19/19 [00:01<00:00, 12.00it/s, loss=0.627, v_num=1.66e+7]\n"
     ]
    }
   ],
   "source": [
    "from src.torch_helpers import NoValProgressBar\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    precision=32,\n",
    "    max_epochs=100,\n",
    "    callbacks=[NoValProgressBar()]\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.predict(model, dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version = hparams['version']\n",
    "\n",
    "!mv ./lightning_logs/version_$SLURM_JOBID ./lightning_logs/cnn_singletask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MSPretraining]",
   "language": "python",
   "name": "conda-env-.conda-MSPretraining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
